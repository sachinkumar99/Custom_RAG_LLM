# -*- coding: utf-8 -*-
"""Custom_RAG_pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vxX6VQab_uAz58RmydPMrzBrTnU7Irs-

Let's install required python libraries
"""

!pip install -q langchain==0.1.11
!pip install -q torch
!pip install -q transformers
!pip install -q sentence-transformers
!pip install -q datasets
!pip install -q faiss-cpu
!pip install -q pypdf
!pip install unstructured
!pip install pdf2image
!pip install pdfminer.six
!pip install pillow_heif
!pip install pikepdf
!pip install unstructured_inference
!pip install pytesseract
!pip install poppler-utils
!apt-get install poppler-utils
!pip install fitz
!pip install pymupdf
!pip install langchainhub

"""Let's import the python libraries"""

from langchain.document_loaders import HuggingFaceDatasetLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from transformers import AutoTokenizer, pipeline
from langchain import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import PyMuPDFLoader

# RAG prompt
from langchain import hub

# Loads the latest version
prompt = hub.pull("rlm/rag-prompt", api_url="https://api.hub.langchain.com")

"""Create the pdf loader instance and load the external book data

---


"""

loader = PyMuPDFLoader('ConceptsofBiology-WEB.pdf')

data = loader.load()



"""We will only consider first two chapters for sentence embeddings and storage. Book pages from 18 to 67 holds this data."""

# Create an instance of the RecursiveCharacterTextSplitter class with specific parameters.
# It splits text into chunks of 1000 characters each with a 150-character overlap.
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)

# 'data' holds the text you want to split, split the text into documents using the text splitter.
docs = text_splitter.split_documents(data[18:68])

"""**Text Embeddings**

**Model - 'all-mpnet-base-v2'**  
It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.
"""

# Define the path to the pre-trained model
modelPath = 'sentence-transformers/all-mpnet-base-v2'

# Create a dictionary with model configuration options, specifying to use the CPU for computations
model_kwargs = {'device':'cpu'}

# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False
encode_kwargs = {'normalize_embeddings': False}

# Initialize an instance of HuggingFaceEmbeddings with the specified parameters
embeddings = HuggingFaceEmbeddings(
    model_name=modelPath,     # Provide the pre-trained model's path
    model_kwargs=model_kwargs, # Pass the model configuration options
    encode_kwargs=encode_kwargs # Pass the encoding options
)

"""**Vector Stores**

I am using FAISS vector storage option by langchain to store embeddings and retreive them
"""

db = FAISS.from_documents(docs, embeddings)

# Let's do similarity search against the vector storage
question = "What is biology?"
searchDocs = db.similarity_search(question)
print(searchDocs[0].page_content)

"""**LLM Model for question answering**

I selected below question answering model from Huggingface based on the leaderboard results!
"""

# Create a tokenizer object by loading the pretrained "Intel/dynamic_tinybert" tokenizer.
tokenizer = AutoTokenizer.from_pretrained('deepset/roberta-base-squad2')

# Create a question-answering model object by loading the pretrained "Intel/dynamic_tinybert" model.
model = AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')

# Specify the model name you want to use
model_name = "deepset/roberta-base-squad2"

# Load the tokenizer associated with the specified model
tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)

# Define a question-answering pipeline using the model and tokenizer
question_answerer = pipeline(
    "question-answering",
    model=model_name,
    tokenizer=tokenizer,
    return_tensors='pt',
)

# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline
# with additional model-specific arguments (temperature and max_length)
llm = HuggingFacePipeline(
    pipeline=question_answerer,
    model_kwargs={"temperature": 0.7, "max_length": 512},
)

# Create a retriever object from the 'db' using the 'as_retriever' method.
# This retriever is likely used for retrieving data or documents from the database.
retriever = db.as_retriever()

docs = retriever.get_relevant_documents("What is biology?")
print(docs[0].page_content)

# Create a retriever object from the 'db' with a search configuration where it retrieves up to 4 relevant splits/documents.
retriever = db.as_retriever(search_kwargs={"k": 2})

# Create a question-answering instance (qa) using the RetrievalQA class.
# It's configured with a language model (llm), a chain type "refine," the retriever we created, and an option to not return source documents.
qa = RetrievalQA.from_chain_type(llm=llm,  retriever=retriever, return_source_documents=False,chain_type_kwargs={"prompt": prompt} )
#chain_type="refine",



"""Note for Evaluators: I spent significant time in fixing below error. The issue is with the prompt template mismatch. Though, I am passing the question correctly, It expects the data in SquadExample format. After my thorough research I beleive this error is due to langchain version compatibility. If I had more time I would have rolled it back and tested it! Thanks

Things I wish I could do and skipped due to time constraints:
1. spend more time cleaning the external data passed.
2. Fine tuning prompt template for the current use case
3. Building streamlit app and hosting this solution.
"""

question = "What is Homeostasis?"
result = qa.run({'query':question})
print(result['result'])



